{"cells":[{"cell_type":"markdown","source":["# Training an MT5 model for multilingual paraphrasing using multilingual paraphrase data in English, German, Czech and Slovene on the whole data\n"],"metadata":{"id":"rBIYKmXSdwrR"}},{"cell_type":"markdown","source":["Main configuration"],"metadata":{"id":"dX4jn4-OhZ1e"}},{"cell_type":"code","source":["initial_finetuning = True  # this is true only at the beginning of fine-tuning. Set to False if you want to continue training from some checkpoint saved on google drive.\n","hf_checkpoint = 'google/mt5-small'\n","drive_checkpoint = ''  # e.g. '/content/drive/MyDrive/models/old-checkpoint-234/'"],"metadata":{"id":"kC59OwICheCK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Environment Setup"],"metadata":{"id":"7ehquyiNedgs"}},{"cell_type":"markdown","source":["We need a GPU, so we check the availability:"],"metadata":{"id":"9xFB_u5EehZZ"}},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"av1Q7GUV-cNY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["And we install all neede libraries."],"metadata":{"id":"2IzhsLc4eoUY"}},{"cell_type":"code","source":["!pip install datasets==2.11.0 transformers==4.28.0 nltk==3.8.1 parascore==1.0.5 sentencepiece==0.1.98"],"metadata":{"id":"4YIx72Wk-50Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We store checkpoints on Google Drive. After we have mounted our Google Drive, the root folder of our Drive is at `/content/drive/MyDrive/`."],"metadata":{"id":"oaFwiuxReskW"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive/\")"],"metadata":{"id":"qIjN13xRjmf_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data Download and Preparation"],"metadata":{"id":"TnGa78O2fB6T"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9f4FhCY_-Z8v"},"outputs":[],"source":["from datasets import load_dataset, interleave_datasets"]},{"cell_type":"markdown","metadata":{"id":"7MvRdOVp-Z8y"},"source":["We use our own created datasets with paraphrases."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hE7imlaA-Z80"},"outputs":[],"source":["raw_dataset = load_dataset('yawnick/para_crawl_multi_all')"]},{"cell_type":"markdown","metadata":{"id":"_5k0OzJs-Z82"},"source":["Let's store the splits separately."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EiSifdWu-Z83"},"outputs":[],"source":["raw_dataset_train = raw_dataset['train']\n","raw_dataset_val = raw_dataset['validation']\n","raw_dataset_test = raw_dataset['test']\n","raw_dataset_train[5]"]},{"cell_type":"markdown","metadata":{"id":"cqrvOpjt-Z88"},"source":["Now, let's prepare the data for training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8pIWb5zX-Z89"},"outputs":[],"source":["from transformers import T5Tokenizer, AutoTokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gLk_1zI7-Z8-"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(hf_checkpoint)"]},{"cell_type":"markdown","metadata":{"id":"Qv0cuJ2H-Z8_"},"source":["Let's see how the tokenizer works:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WWwynbLR-Z8_"},"outputs":[],"source":["s1 = raw_dataset_train[4]['Original']\n","s2 = raw_dataset_train[4]['Paraphrase']\n","print(s1)\n","print(s2)\n","inputs = tokenizer(s1, text_target=s2)\n","print([tokenizer.decode(id) for id in inputs['input_ids']])\n","inputs"]},{"cell_type":"markdown","metadata":{"id":"ORBVjjfU-Z9A"},"source":["Now we create a preprocess function that turns a dataset item into a form that the model can use for training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tSsGVnTq-Z9C"},"outputs":[],"source":["max_length = 128\n","\n","# the prefix has to (dynamically) be adjusted depending on the language or when training multilingually (I think).\n","prefix = 'paraphrase: '\n","\n","def preprocess_function(examples):\n","    inputs = [prefix+s1 for s1 in examples['Original']]\n","    targets = examples['Paraphrase']\n","    # most likely there will be nothing to truncate, but we still add it\n","    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True)\n","    return model_inputs"]},{"cell_type":"markdown","metadata":{"id":"IaLNyvpj-Z9C"},"source":["Now we apply the preprocessing function to the datasets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9_haMBAZ-Z9C"},"outputs":[],"source":["tokenized_ds_train = raw_dataset_train.map(\n","    preprocess_function,\n","    batched=True,\n","    remove_columns=raw_dataset_train.column_names\n",")\n","tokenized_ds_val = raw_dataset_val.map(\n","    preprocess_function,\n","    batched=True,\n","    remove_columns=raw_dataset_val.column_names\n",")\n","tokenized_ds_test = raw_dataset_test.map(\n","    preprocess_function,\n","    batched=True,\n","    remove_columns=raw_dataset_test.column_names\n",")"]},{"cell_type":"markdown","metadata":{"id":"r7VjTLAa-Z9D"},"source":["Now the data is ready."]},{"cell_type":"markdown","source":["## Model and Training Preparation"],"metadata":{"id":"9fRrvix7gCWD"}},{"cell_type":"markdown","metadata":{"id":"sMf3Aj4g-Z9E"},"source":["Next, the model and a Datacollator."]},{"cell_type":"code","source":["from transformers import MT5ForConditionalGeneration"],"metadata":{"id":"ZWy1Pf-kcw35"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Either load the pretrained model from huggingface at the beginning of fine-tuning for the first epoch, or load the model from a previous fine-tune checkooint from google drive."],"metadata":{"id":"owRok1yyDSLk"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"zM07lJxK-Z9E"},"outputs":[],"source":["if initial_finetuning:\n","  model = MT5ForConditionalGeneration.from_pretrained(hf_checkpoint)\n","else:\n","  model = MT5ForConditionalGeneration.from_pretrained(drive_checkpoint)"]},{"cell_type":"markdown","source":["Next, we instantiate a DataCollator."],"metadata":{"id":"ulA-c68DiL6g"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"8XMqjDhK-Z9E"},"outputs":[],"source":["from transformers import DataCollatorForSeq2Seq\n","\n","data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"]},{"cell_type":"markdown","metadata":{"id":"M15NFiNX-Z9E"},"source":["Here, I'll skip the example usage of the datacollator, check it out [here](https://huggingface.co/learn/nlp-course/chapter7/4?fw=pt#data-collation)."]},{"cell_type":"markdown","metadata":{"id":"ipl22AtD-Z9F"},"source":["Now, let's continue with metrics. We will use Parascore."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"irzhzbqv-Z9F"},"outputs":[],"source":["from parascore import ParaScorer\n","\n","scorer = ParaScorer(lang='multi')"]},{"cell_type":"markdown","metadata":{"id":"iXr4ffTb-Z9F"},"source":["Let's quickly go over how Parascore is used (this example is in english, so it's not ideal):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e8C6DHFs-Z9G"},"outputs":[],"source":["cands = [\"A young person is skating.\", \"I like sports.\", \"He catches the ball.\", \"That's very interesting!\"]\n","sources = [\"There's a child on a skateboard.\", \"I like to relax.\", \"good morning, everyone!\", \"I find this interesting.\"]\n","score = scorer.free_score(cands, sources)\n","float(score[-1].mean())"]},{"cell_type":"markdown","metadata":{"id":"lYrb1ozE-Z9G"},"source":["Now, here's the `compute_metrics` function (mostly copied from [here](https://huggingface.co/learn/nlp-course/chapter7/4?fw=pt#metrics)):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IHgPnQAM-Z9G"},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b0hqKeye-Z9G"},"outputs":[],"source":["def compute_metrics(eval_preds):\n","    preds, labels = eval_preds\n","    # In case the model returns more than the prediction logits\n","    if isinstance(preds, tuple):\n","        preds = preds[0]\n","\n","    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n","\n","    # Replace -100s in the labels as we can't decode them\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    # Some simple post-processing\n","    decoded_preds = [pred.strip() for pred in decoded_preds]\n","    decoded_labels = [label.strip() for label in decoded_labels]\n","    print(decoded_preds[:5])\n","    print(decoded_labels[:5])\n","    \n","    parascore = scorer.free_score(decoded_preds, decoded_labels)\n","    return {'parascore': float(parascore[-1].mean())}\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qwDfBotb-Z9H"},"outputs":[],"source":["from transformers import Seq2SeqTrainingArguments\n","from transformers import Seq2SeqTrainer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"caBcGcNG-Z9H"},"outputs":[],"source":["\n","\n","args = Seq2SeqTrainingArguments(\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    logging_strategy='steps',\n","    logging_steps=500,\n","    output_dir='/content/drive/MyDrive/models/multi-para-all',  # this is where the checkpoint will be saved\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    weight_decay=0.01,\n","    save_total_limit=5,\n","    num_train_epochs=5,\n","    predict_with_generate=True,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yEFcQrRa-Z9I"},"outputs":[],"source":["trainer = Seq2SeqTrainer(\n","    model,\n","    args,\n","    train_dataset=tokenized_ds_train,\n","    eval_dataset=tokenized_ds_val,\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F-lIyqSn-Z9I"},"outputs":[],"source":["trainer.train()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"orig_nbformat":4,"colab":{"provenance":[]},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}